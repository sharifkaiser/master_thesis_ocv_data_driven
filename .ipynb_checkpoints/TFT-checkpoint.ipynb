{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3448f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:21:57.747496Z",
     "start_time": "2022-08-21T17:21:57.743410Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8861d531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:21:58.881749Z",
     "start_time": "2022-08-21T17:21:58.874565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase default window size for notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b949d8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:21:59.907186Z",
     "start_time": "2022-08-21T17:21:59.903096Z"
    }
   },
   "outputs": [],
   "source": [
    "# use pytorch-forecasting\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb969eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:11.518397Z",
     "start_time": "2022-08-21T17:22:00.552636Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88afc9b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:12.574262Z",
     "start_time": "2022-08-21T17:22:12.567983Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd = sys.path[0]\n",
    "sys.path.append(os.path.join(cwd, 'my_modules'))                # sys.path[0] is dir of the ipynb file\n",
    "import custom_plot\n",
    "import data_clean\n",
    "import data_preprocess\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8b88336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:14.545683Z",
     "start_time": "2022-08-21T17:22:14.503533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n",
      "pandas version: 1.4.2\n",
      "numpy version: 1.21.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "from dataclasses import dataclass       # C like structure\n",
    "import glob                             # finds all the pathnames matching specified pattern\n",
    "import datetime as dt\n",
    "import random\n",
    "!python --version\n",
    "print('pandas version: ' + pd.__version__)\n",
    "print('numpy version: ' + np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf4a0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:16.381907Z",
     "start_time": "2022-08-21T17:22:16.376398Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Plot related packages,%matplotlib notebook makes plots in jupyter interactive\n",
    "# constants for plotting\n",
    "x_label_elapsedtime = 0\n",
    "x_label_datetime = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bc60ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:18.611884Z",
     "start_time": "2022-08-21T17:22:18.456466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load processed data (phase 1)\n",
    "\n",
    "dir_path = os.path.join(cwd, 'csv')\n",
    "filename = 'cell_cycles.pkl'\n",
    "\n",
    "# load last saved df from phase1 data and convert time stamp and sort\n",
    "li_ts_cycles_ph1 = data_preprocess.load_object(dir_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b5c946d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:19.581399Z",
     "start_time": "2022-08-21T17:22:19.576489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load processed data (phase 2)\n",
    "dir_path = os.path.join(cwd, 'csv', 'phase_2_pkl')\n",
    "filename = 'mod1_cell_cycles.pkl'\n",
    "li_ts_cycles_ph2 = data_preprocess.load_object(dir_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2647ecfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:20.593967Z",
     "start_time": "2022-08-21T17:22:20.576797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined cells V1\n",
      "combined cells V2\n",
      "combined cells V3\n",
      "combined cells V4\n",
      "combined cells V5\n",
      "combined cells V6\n",
      "combined cells V7\n",
      "combined cells V8\n",
      "combined cells V9\n",
      "combined cells V10\n",
      "combined cells V11\n",
      "combined cells V12\n"
     ]
    }
   ],
   "source": [
    "# Combine phase1 and phase 2 data\n",
    "import math\n",
    "\n",
    "# for x, y in li_ts_cycles_ph1:\n",
    "#     print('phase1 cells', x, y)\n",
    "    \n",
    "# for x, y in li_ts_cycles_ph2:\n",
    "#     print('phase2 cells', len(y))\n",
    "\n",
    "combined_phase_cycles = []\n",
    "if len(li_ts_cycles_ph1) <= len(li_ts_cycles_ph2):\n",
    "    for i in range(len(li_ts_cycles_ph1)):              # each cell contain multiple cycles\n",
    "        c_1, cycles_1 = li_ts_cycles_ph1[i]                 # for example, (v1, [cycles])\n",
    "        c_2, cycles_2 = li_ts_cycles_ph2[i]\n",
    "        if (c_2 == c_1):\n",
    "            cycles_1 += cycles_2                        # this adds cycle to li_ts_cycles_ph1\n",
    "    combined_phase_cycles = li_ts_cycles_ph1\n",
    "else:\n",
    "    for i in range(len(li_ts_cycles_ph2)):              # each cell contain multiple cycles\n",
    "        c_2, cycles_2 = li_ts_cycles_ph2[i]\n",
    "        c_1, cycles_1 = li_ts_cycles_ph1[i]\n",
    "        if (c_2 == c_1):\n",
    "            cycles_2 += cycles_1                        # this adds cycle to li_ts_cycles\n",
    "    combined_phase_cycles = li_ts_cycles_ph2\n",
    "\n",
    "\n",
    "for x, y in combined_phase_cycles:\n",
    "    print('combined cells', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a842cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:23.263537Z",
     "start_time": "2022-08-21T17:22:23.256152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 15 2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Separate training, validation and test cycles, keep cycles without cell ID\n",
    "TRAIN_SAMPLES = 0.9\n",
    "li_train_cycles = []      # each of this list is a separate static_dynamic_static cycle\n",
    "# li_validation_cycles = []\n",
    "li_test_cycles= []\n",
    "counter = 0\n",
    "\n",
    "total_cycles = 0\n",
    "for (c, li_cycles) in combined_phase_cycles:              # each cell contain multiple cycles\n",
    "    counter += 1\n",
    "    num_cell_cycles = len(li_cycles)\n",
    "    num_train_cycles = math.floor(num_cell_cycles * TRAIN_SAMPLES)                 # training cycle samples 70%\n",
    "    # num_validation_cycles = math.ceil((num_cell_cycles - num_train_cycles) * 0.5)\n",
    "    # num_test_cycles = num_cell_cycles - num_validation_cycles - num_train_cycles\n",
    "    num_test_cycles = num_cell_cycles - num_train_cycles\n",
    "    total_cycles += len(li_cycles)\n",
    "    # print(num_cell_cycles, num_train_cycles, num_validation_cycles, num_test_cycles)\n",
    "\n",
    "    li_train_cycles += li_cycles[0:num_train_cycles]\n",
    "    # li_validation_cycles += li_cycles[num_train_cycles:num_train_cycles+num_validation_cycles]\n",
    "    # li_test_cycles += li_cycles[num_train_cycles+num_validation_cycles:]\n",
    "    li_test_cycles += li_cycles[num_train_cycles:]\n",
    "#     if (counter == 7):\n",
    "#         break\n",
    "    break\n",
    "\n",
    "print(total_cycles, len(li_train_cycles), len(li_test_cycles))     # test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bda6d4",
   "metadata": {},
   "source": [
    "### univariate: current to voltage mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3cee02b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:24.948958Z",
     "start_time": "2022-08-21T17:22:24.943492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make dataset ready for LSTM\n",
    "# [temporal input sequence] [output]\n",
    "# example:\n",
    "# [[[1], [2], [3], [4], [5]]] [6]\n",
    "# [[[2], [3], [4], [5], [6]]] [7]\n",
    "# [[[3], [4], [5], [6], [7]]] [8]\n",
    "\n",
    "def df_to_supervised_univariate(df_feature, df_label, window_size=1):\n",
    "    df_feature_as_np = df_feature.to_numpy()\n",
    "    df_label_as_np = df_label.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    total_rows = len(df_feature_as_np) - window_size + 1\n",
    "    for i in range(len(df_feature_as_np) - window_size):\n",
    "        row = [[a] for a in df_feature_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        label = df_label_as_np[i+window_size]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87fdb028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:28.427542Z",
     "start_time": "2022-08-21T17:22:28.408366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             V  current   Temp  elapsed_sec  V_standard  cur_standard  \\\n",
      "695111  4.1432      0.0  14.25            0    2.186559      0.776016   \n",
      "695112  4.1432      0.0  14.25            1    2.186559      0.776016   \n",
      "695113  4.1432      0.0  14.25            2    2.186559      0.776016   \n",
      "695114  4.1432      0.0  14.25            3    2.186559      0.776016   \n",
      "695115  4.1432      0.0  14.25            4    2.186559      0.776016   \n",
      "...        ...      ...    ...          ...         ...           ...   \n",
      "696599  4.0188      0.0  15.00         1488   -0.420828      0.776016   \n",
      "696600  4.0188      0.0  15.00         1489   -0.420828      0.776016   \n",
      "696601  4.0189      0.0  15.00         1490   -0.418732      0.776016   \n",
      "696602  4.0189      0.0  15.00         1491   -0.418732      0.776016   \n",
      "696603  4.0189      0.0  15.00         1492   -0.418732      0.776016   \n",
      "\n",
      "        V_normal  cur_normal  \n",
      "695111  1.000000    0.805672  \n",
      "695112  1.000000    0.805672  \n",
      "695113  1.000000    0.805672  \n",
      "695114  1.000000    0.805672  \n",
      "695115  1.000000    0.805672  \n",
      "...          ...         ...  \n",
      "696599  0.590655    0.805672  \n",
      "696600  0.590655    0.805672  \n",
      "696601  0.590984    0.805672  \n",
      "696602  0.590984    0.805672  \n",
      "696603  0.590984    0.805672  \n",
      "\n",
      "[1493 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "TIMESTEPS = 10\n",
    "# One sample only\n",
    "df = li_train_cycles[2].copy()\n",
    "df = df[['V', 'current', 'Temp', 'elapsed_sec']]\n",
    "\n",
    "# statndard\n",
    "df['V_standard'] = StandardScaler().fit_transform(df[['V']])\n",
    "df['cur_standard'] = StandardScaler().fit_transform(df[['current']])\n",
    "\n",
    "# normnal\n",
    "df['V_normal'] = MinMaxScaler().fit_transform(df[['V']])\n",
    "df['cur_normal'] = MinMaxScaler().fit_transform(df[['current']])\n",
    "\n",
    "df['elapsed_sec'] = df['elapsed_sec'].astype(int)\n",
    "\n",
    "# print(df.describe())\n",
    "print(df)\n",
    "\n",
    "# reverse data before training, for test\n",
    "# df = df.iloc[::-1]\n",
    "# df1 = df.loc[df['contactor_state'] == 2]               # take only dynamic voltage\n",
    "\n",
    "# X, y = df_to_supervised_univariate(df['cur_standard'], df['V_standard'], TIMESTEPS)     # standard\n",
    "# X, y = df_to_supervised_univariate(df['cur_normal'], df['V_normal'], TIMESTEPS)     # normal\n",
    "# X, y = df_to_supervised_univariate(df['current'], df['V'], TIMESTEPS)     # without normalization\n",
    "# print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd48e291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:22:49.489077Z",
     "start_time": "2022-08-21T17:22:49.440062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n",
      "pytorch: 1.11.0\n",
      "pytorch_lightning: 1.6.5\n",
      "pytorch_forecasting: 0.10.2\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Import libraries\n",
    "###########\n",
    "\n",
    "# Basic Python\n",
    "import sys # Python sys functions\n",
    "import os  # Python os functions\n",
    "import time # Python time functions\n",
    "import typing #Python types\n",
    "import logging # Python logging functions\n",
    "import warnings # Python warnings\n",
    "warnings.filterwarnings(\"ignore\")  # don't show warnings\n",
    "# import fastparquet  # Engine for parquet support\n",
    "# import GPUtil #GPU status from NVIDA GPUs\n",
    "\n",
    "# Open-source libraries:\n",
    "import numpy as np # Numerical processing\n",
    "import pandas as pd  # Dataframe (tabular data) processing\n",
    "# import matplotlib # Graph plotting\n",
    "from matplotlib import pyplot as plt # Graph plotting\n",
    "# stop warnings from pyotorch_forecasting too many open plots\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "# import ray                # Run distributed code\n",
    "# from ray_lightning import RayPlugin #Ray plugin to parallelize Pytorch Lightning\n",
    "# from ray.train import Trainer # Ray library for other AI libraries\n",
    "\n",
    "# PyTorch, PyTorch Lightning, and PyTorch Forecasting\n",
    "import torch  #Pytorch\n",
    "import pytorch_lightning as pl  #PyTorch Lightning convenience APIs for PyTorch\n",
    "import pytorch_forecasting as ptf #PyTorch Forecasting convenience APIs for PyTorch Lightning\n",
    "pl.seed_everything(415)  # Set global random seed\n",
    "\n",
    "# PyTorch visualization uses Tensorboard\n",
    "import tensorflow as tf #Tensorflow\n",
    "import tensorboard as tb  #Tensorboard\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile  #compatibility for PyTorch\n",
    "\n",
    "# TODO remove this\n",
    "# Override pytorch_forecasting with my copy\n",
    "sys.path.insert( 0, os.path.abspath(\"../githubPublicPytorchForecasting/my-copy-pytorch-forecasting\") )\n",
    "import pytorch_forecasting as ptf \n",
    "\n",
    "\n",
    "!python --version\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "print(f\"pytorch_lightning: {pl.__version__}\")\n",
    "print(f\"pytorch_forecasting: {ptf.__version__}\")\n",
    "# print(f\"ray: {ray.__version__}\")\n",
    "# print(f\"gputil: {GPUtil.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0402207e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T17:24:04.565615Z",
     "start_time": "2022-08-21T17:24:04.561620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# Todo: Move functions inside util.py\n",
    "\n",
    "# Convert data from pandas to PyTorch tensors.\n",
    "def convert_pandas_pytorch_timeseriesdata(\n",
    "    input_data_pandas_df:pd.DataFrame, \n",
    "    config:dict\n",
    ") -> typing.Union['pytorch_forecasting.data.timeseries.TimeSeriesDataSet',\n",
    "                  'torch.utils.data.dataloader.DataLoader']:\n",
    "\n",
    "    \"\"\"Converts pandas dataframe into TimeSeries folded tensors following \n",
    "       the backtesting technique.  A generator for doing the folding is \n",
    "       per batch also created.  One for the training data.  \n",
    "       Another for the validation data.  \n",
    "\n",
    "    Inputs:\n",
    "        pd.DataFrame: All the input data\n",
    "        dict: config is a configuration file containing hard-coded settings.\n",
    "\n",
    "    Returns:\n",
    "        'pytorch_forecasting.data.timeseries.TimeSeriesDataSet': training data\n",
    "        'torch.utils.data.dataloader.DataLoader': training data loader\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "    \"\"\"\n",
    "    \n",
    "    # specify data parameters\n",
    "    FORECAST_HORIZON = config.get(\"forecast_horizon\", 168)\n",
    "    CONTEXT_LENGTH = config.get(\"context_length\", 63)\n",
    "    BATCH_SIZE = config.get(\"batch_size\", 32)\n",
    "    NUM_TRAINING_WORKERS = config.get(\"num_training_workers\", 4)\n",
    "    id_col_name = \"pulocationid\"\n",
    "    target_value = \"trip_quantity\"\n",
    "    \n",
    "    the_df = input_data_pandas_df.copy()\n",
    "    \n",
    "    # define forecast horizon and training cutoff\n",
    "    max_prediction_length = FORECAST_HORIZON  #decoder length = 1 week forecast horizon\n",
    "    max_encoder_length = CONTEXT_LENGTH  # window or context length\n",
    "    training_cutoff = the_df[\"time_idx\"].max() - max_prediction_length \n",
    "\n",
    "    # convert pandas to PyTorch tensor\n",
    "    training_data = ptf.data.TimeSeriesDataSet(\n",
    "        the_df[lambda x: x.time_idx <= training_cutoff],\n",
    "        allow_missing_timesteps=True,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_value,\n",
    "        group_ids=[id_col_name],\n",
    "        min_encoder_length=5,  # min 5 historical values must exist\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[id_col_name],\n",
    "        # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[\"day_hour\"],\n",
    "        # group of categorical variables can be treated as one variable\n",
    "        # variable_groups={\"special_days\": special_days},  \n",
    "        time_varying_known_reals=[\"time_idx\", ],\n",
    "                            # \"mean_item_loc_weekday\",\n",
    "                            # \"binned_max_item\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[target_value,],\n",
    "\n",
    "        # https://pytorch-forecasting.readthedocs.io/en/v0.2.4/_modules/pytorch_forecasting/data.html\n",
    "        target_normalizer=ptf.data.GroupNormalizer(\n",
    "            groups=[\"pulocationid\"], \n",
    "            transformation=\"softplus\"  #forces positive values\n",
    "        ), \n",
    "        add_relative_time_idx=True, # add as feature\n",
    "        add_target_scales=True, # add avg target_value as feature\n",
    "        add_encoder_length=True, # add as feature\n",
    "    )\n",
    "    \n",
    "    # create PyTorch dataloader for training\n",
    "    train_loader = training_data\\\n",
    "                        .to_dataloader(\n",
    "                            train=True, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # create validation PyTorch data \n",
    "    # (predict=True) means make do inference using the validation data\n",
    "    val_dataset = ptf.data.TimeSeriesDataSet\\\n",
    "                    .from_dataset(\n",
    "                        training_data, \n",
    "                        data=the_df, \n",
    "                        predict=True, \n",
    "                        stop_randomization=True)\n",
    "\n",
    "    # create PyTorch dataloaders for inference on validation data\n",
    "    validation_loader = val_dataset\\\n",
    "                    .to_dataloader(\n",
    "                        train=False, \n",
    "                        batch_size=BATCH_SIZE * 10, \n",
    "                        num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # return original df converted to PyTorch tensors, and pytorch loaders\n",
    "    return training_data, train_loader, validation_loader\n",
    "\n",
    "# Define a PyTorch Lightning TemporalFusionTransformer model\n",
    "def define_pytorch_model(\n",
    "    train_dataset: 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet', \n",
    "    config: dict, \n",
    "    ray_plugin: 'ray_lightning.ray_ddp.RayPlugin'\n",
    ") -> typing.Union['pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer',\n",
    "                  'pytorch_lightning.trainer.trainer.Trainer']:\n",
    "\n",
    "    \"\"\"Define a PyTorch Lightning TemporalFusionTransformer model and a \n",
    "       PyTorch Lightning trainer.  Initial values for the model are hard-\n",
    "       coded in the config dictionary.\n",
    "\n",
    "    Returns:\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer': model\n",
    "        'pytorch_lightning.trainer.trainer.Trainer': trainer for fitting the model\n",
    "    \"\"\"  \n",
    "    # get the parameters from config\n",
    "    NUM_GPU = config.get(\"num_gpus\", 0)\n",
    "    EPOCHS = config.get(\"epochs\", 30)\n",
    "    LR = config.get(\"lr\", 0.01)\n",
    "    HIDDEN_SIZE = config.get(\"hidden_size\", 40)\n",
    "    ATTENTION_HEAD_SIZE = config.get(\"attention_head_size\", 4)\n",
    "    HIDDEN_CONTINUOUS_SIZE = config.get(\"hidden_continuous_size\", 1)\n",
    "    DROPOUT = config.get(\"droupout\", 0.1)\n",
    "    LIMIT_TRAIN_BATCHES = config.get(\"limit_train_batches\", 30)\n",
    "    FAST_MODE = config.get(\"fast_mode\", False)\n",
    "    TUNING_RUN = config.get(\"tuning_run\", False)\n",
    "    \n",
    "    print(f\"learning_rate = {LR}\")\n",
    "    print(f\"hidden_size = {HIDDEN_SIZE}\")\n",
    "    print(f\"attention_head_size = {ATTENTION_HEAD_SIZE}\")\n",
    "    print(f\"hidden_continuous_size = {HIDDEN_CONTINUOUS_SIZE}\")\n",
    "    print(f\"limit_train_batches = {LIMIT_TRAIN_BATCHES}\")\n",
    "    \n",
    "    if ray_plugin is None:\n",
    "        PLUGINS = []\n",
    "    else:\n",
    "        PLUGINS=[ray_plugin]\n",
    "\n",
    "    # configure early stopping when validation loss does not improve \n",
    "    early_stop_callback = \\\n",
    "        pl.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            min_delta=1e-4, \n",
    "            patience=10,   #1\n",
    "            verbose=False, \n",
    "            mode=\"min\")\n",
    "    \n",
    "    # Create the Tune Reporting Callback\n",
    "    metrics = {\"loss\": \"ptf.metrics.QuantileLoss()\"}\n",
    "    tune_callback = \\\n",
    "        ray.tune.integration.pytorch_lightning.TuneReportCallback(\n",
    "            metrics, \n",
    "            on=\"validation_end\")\n",
    "    \n",
    "    # configure logging\n",
    "    lr_logger = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs\")  # log results to a tensorboard\n",
    "        \n",
    "    # Define callbacks based on passed parameter to run tuning or not\n",
    "    if TUNING_RUN:\n",
    "        CALLBACKS = [lr_logger, early_stop_callback, tune_callback]\n",
    "    else:\n",
    "        CALLBACKS = [lr_logger, early_stop_callback]\n",
    "        \n",
    "    # configure PyTorch trainer with Ray Lightning plugin\n",
    "    torch_trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        gpus=NUM_GPU,\n",
    "        # weights_summary=\"top\",\n",
    "        \n",
    "        # The value at which to clip gradients. \n",
    "        # Passing gradient_clip_val=None disables gradient clipping. \n",
    "        gradient_clip_val=0.1, \n",
    "        \n",
    "        # Number of batches or percent size of training data each epoch\n",
    "        # limit_train_batches=30,  #use 30 batches of training data each epoch \n",
    "        limit_train_batches=LIMIT_TRAIN_BATCHES,  \n",
    "        \n",
    "        # how often to log, default=50\n",
    "        logger=logger,\n",
    "        # log_every_n_steps=500,  #default 50\n",
    "        \n",
    "        # sanity check runs n batches of val before starting the training, default=2\n",
    "        # num_sanity_val_steps=1,\n",
    "        \n",
    "        # Callbacks run sequentially in the order defined here.  Except ModelCheckpoint callback\n",
    "        # runs after all others to ensure all states are saved to the checkpoints.\n",
    "        callbacks=CALLBACKS,\n",
    "        \n",
    "        # Run \"fast\" mode for quick sanity check\n",
    "        # Note: No trainer checkpoints will be saved in fast mode\n",
    "        fast_dev_run=FAST_MODE,\n",
    "        \n",
    "        # This is the Ray parallelizing distributed part\n",
    "        # regular python - just comment out below line\n",
    "        plugins = PLUGINS\n",
    "    )\n",
    "    if not FAST_MODE:\n",
    "        print(f\"checkpoints location: {torch_trainer.logger.log_dir}\")\n",
    "    \n",
    "\n",
    "    # initialize the model\n",
    "    tft = ptf.models.TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN_SIZE, # num neurons in each layer, bigger runs more slowly\n",
    "        # lstm_layers=HIDDEN_LAYERS, #LSTM layers=1 #default=1 for tft architecture\n",
    "        attention_head_size=ATTENTION_HEAD_SIZE,  #default 4 cells in LSTM layer\n",
    "        dropout=DROPOUT,\n",
    "        hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,  #similar to categorical embedding size\n",
    "        # 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "        # output_size=7,  \n",
    "        # optimizer loss metric\n",
    "        loss=ptf.metrics.QuantileLoss(),\n",
    "        # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        log_interval=50,  #50\n",
    "        reduce_on_plateau_patience=4, # reduce learning automatically\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "    \n",
    "    # return the model and trainer\n",
    "    return tft, torch_trainer\n",
    "\n",
    "# Define a calling function to read data, define model, train it\n",
    "def train_func(config: dict, \n",
    "               ray_plugin: 'ray_lightning.ray_ddp.RayPlugin'\n",
    ") -> typing.Union['torch.utils.data.dataloader.DataLoader',\n",
    "        'pytorch_lightning.trainer.trainer.Trainer',\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer',\n",
    "         str]:\n",
    "    \"\"\"Define a calling function to read data, define a model and train it.\n",
    "\n",
    "    Inputs:\n",
    "        dict: configuration dictionary with hard-coded runtime values\n",
    "        'ray_lightning.ray_ddp.RayPlugin': plugin for PyTorch Lightning trainer\n",
    "\n",
    "    Returns:\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "        'pytorch_lightning.trainer.trainer.Trainer': trainer for fitting the model\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer': trained model\n",
    "        str: path where Pytorch Forecasting model is stored\n",
    "    \"\"\"\n",
    "    # # stop warnings from pyotorch_forecasting too many open plots\n",
    "    # matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "    \n",
    "    # read data into pandas dataframe\n",
    "    filename = \"data/clean_taxi_hourly.parquet\"\n",
    "    df = pd.read_parquet(filename)\n",
    "    df = df[[\"time_idx\", \"pulocationid\", \"day_hour\",\n",
    "                 \"trip_quantity\", \"mean_item_loc_weekday\",\n",
    "                 \"binned_max_item\"]].copy()\n",
    "\n",
    "    # convert data from pandas to PyTorch tensors\n",
    "    train_dataset, train_loader, validation_loader = \\\n",
    "        convert_pandas_pytorch_timeseriesdata(df, config)\n",
    "\n",
    "    # define a PyTorch deep learning forecasting model\n",
    "    model, trainer = define_pytorch_model(\n",
    "                                           train_dataset, \n",
    "                                           config,\n",
    "                                           ray_plugin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e36d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-21T12:51:56.375948Z",
     "start_time": "2022-08-21T12:51:56.296358Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "target V should be an unknown continuous variable in the future",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m max_encoder_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[0;32m      5\u001b[0m training_cutoff \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m max_prediction_length\n\u001b[1;32m----> 7\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melapsed_sec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_cutoff\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melapsed_sec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# voltage\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# keep encoder length long (as it is in the validation set)\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# no static in my data, static = does not vary with time\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# group of categorical variables can be treated as one variable\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;43;03m#     target_normalizer=GroupNormalizer(\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;43;03m#         groups=[\"agency\", \"sku\"], transformation=\"softplus\"\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;43;03m#     ),  # use softplus and normalize by group\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_relative_time_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_target_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# create validation set (predict=True) which means to predict the last max_prediction_length points in time\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# for each series\u001b[39;00m\n\u001b[0;32m     36\u001b[0m validation \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet\u001b[38;5;241m.\u001b[39mfrom_dataset(training, df, predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, stop_randomization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:359\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[1;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_overwrite_values()\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_names:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    360\u001b[0m         target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_varying_known_reals\n\u001b[0;32m    361\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be an unknown continuous variable in the future\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# add time index relative to prediction position\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_relative_time_idx \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_encoder_length:\n",
      "\u001b[1;31mAssertionError\u001b[0m: target V should be an unknown continuous variable in the future"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloaders\n",
    "\n",
    "max_prediction_length = 300                # last 300 seconds as a validation set\n",
    "max_encoder_length = 24\n",
    "training_cutoff = df[\"elapsed_sec\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[lambda x: x[\"elapsed_sec\"] <= training_cutoff],\n",
    "    time_idx=\"elapsed_sec\",\n",
    "    target=\"V\",    # voltage\n",
    "    group_ids=[\"current\", \"V\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],       # no static in my data, static = does not vary with time\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    variable_groups={},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"V\", \"current\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"V\",\n",
    "        \"current\",\n",
    "    ],\n",
    "#     target_normalizer=GroupNormalizer(\n",
    "#         groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "#     ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
